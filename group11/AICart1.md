# 🏎️ 自動駕駛強化學習訓練報告

## 👥 組別與組員  
**組別**：第11組 
**組員名單**：  
- 楊少禎 108360224 
- 陳俊吉 111360203  
- 劉品辰 111360110 

---
📘 **專案簡介**
本專案旨在透過 AWS DeepRacer 平台與強化學習（Reinforcement Learning, RL）技術，訓練一輛虛擬自駕車在賽道上完成高速且穩定的繞圈。訓練過程中我們設計 reward function 引導模型學習理想的駕駛行為，並分為四個階段逐步優化。

使用的學習方法為 **PPO (Proximal Policy Optimization)** 為基礎的強化學習架構（從圖片中的「強化學習算法 PPO」判斷），透過反覆試誤方式學習出在不同賽道位置應採取的最佳行動（Action），並根據我們定義的 reward 函數調整學習策略。



## 🛠️ 訓練技巧分享

### 初期訓練觀察重點：
- 確保車輛開始能貼近賽道，避免總是偏離或打轉
- 若 early reward 不穩，考慮降低 learning_rate 或放寬懲罰條件

### 獎勵函數調整流程：
- 每次只改一處條件，觀察 reward 曲線與車輛行為
- 可用視覺化工具對比不同版本表現（如貼線程度、速度分布）

### 速度調控策略：
- 搭配 waypoints 區段分類調整速度門檻
- 若 steering_angle 過大，給予速度懲罰避免過彎失控

### 完賽優化：
設計完成獎勵與步數掛鉤，例如：

```python
reward += COMPLETION_BONUS / (steps - TARGET_STEP)


🧭 **訓練目標與評估指標**
我們的訓練重點圍繞以下四大核心：

* **穩定性**：保持四輪在賽道內，避免出界。
* **路徑優化**：靠近中心線行駛以減少偏移與碰撞風險。
* **動作平滑性**：降低急轉彎（過大角度）操作。
* **速度效率**：鼓勵適中偏快的速度（避免太慢或太快造成失控）。

評估指標包含：

* **單圈完成時間**（越短越佳）
* **成功繞圈率**（不出界的圈數比率）
* **模型穩定度**（行為一致性）

🌀 **第一階段：基礎穩定訓練**
目標：建立基本能力，完成單圈不出界。

reward 設計重點：

* `all_wheels_on_track` 為否時直接給極低獎勵。
* 使用 `distance_from_center` 分為三段距離評分，越靠近中心獎勵越高。
* 若轉彎角度過大（>20°）開始懲罰。
* 太慢（<1.2 m/s）則給獎勵，鼓勵「穩中求快」。

結果：

* 模型可穩定完成單圈，無出界情形。
* **從圖片  中可以看到，獎勵分數（綠線）從一開始的約 26.50 逐步上升，完成百分比（訓練，藍線）穩定在 8.88% 左右，而完成百分比（評估，紅線）穩定在 3.00% 左右。這表示模型在訓練初期已能穩定地在賽道上行駛，儘管評估完成度較低（可能由於訓練初期策略較為保守，未達成完整的圈數）。在超參數方面，梯度下降批量大小（Gradient descent batch size）為 128，循環處理次數（Number of epochs）為 10，學習率（Learning rate）為 0.0003。動作空間中，轉向角為 -25° 時對應速度為 1 m/s、2 m/s、3 m/s。**
* 單圈時間約 25 秒，偏慢但安全。

![第一階段訓練圖表]
![alt text](image-3.png)

🚧 **第二階段：速度提升嘗試**
目標：縮短單圈時間，提升速度表現。

reward 設計重點：

* 修改速度獎勵區間，鼓勵達 1.8~2.5 m/s。
* 降低對角度懲罰的權重，讓模型能承受一定程度的激進操作。
* 保留中心線距離獎勵作為方向參考。

結果：

* 單圈速度有明顯提升，但導致大量出界與失控。
* 成功率下降至約 30~40%。**從圖片的圖表可見，完成百分比（評估）的紅線波動劇烈，且多數位於 50% 以下，印證了成功率下降的描述。最佳單圈時間（Best Lap）為 00:10.860，上次單圈時間（Last Lap）為 00:11.067，顯示單圈時間顯著縮短。**
* 在模型參數中，學習率（Learning rate）為 0.0003，梯度下降批量大小（Gradient descent batch size）為 64。

📸 **圖片參考：**
![第二階段訓練圖表]
![alt text](image-5.png)
⚖️ **第三階段：速度與穩定性權衡**
目標：在保持較高速度的前提下回穩模型表現。

reward 設計重點：

* 明確限制速度與轉向角的搭配，例如「高速 + 急轉」給予高懲罰。
* 建立精細的速度區間獎勵：
    * 1.5~2.5 m/s 給予額外獎勵
    * <1.0 m/s 給予懲罰以避免過慢
* 中心線靠近邏輯進行微調，提升對偏離過多的懲罰力度。

結果：

* 單圈完成時間約 15~18 秒，表現較穩。
* 成功率上升至 70~80%，但模型行為仍不完全穩定。**從圖片的圖表可見，完成百分比（評估）的紅線在多數迭代中上升並維持在 75% 左右，獎勵分數（綠線）也相對穩定地提升。最佳單圈時間（Best Lap）為 00:07.870，上次單圈時間（Last Lap）為 00:08.955，顯示速度進一步提升且相對穩定。**

📸 **圖片參考：**
![第三階段訓練圖表](image_5d8abe.jpg)


📊 **訓練階段比較一覽表**
| 階段   | 單圈時間 | 成功率  | 特徵             |
| :----- | :------- | :------ | :--------------- |
| 第1階段 | 約 25 秒 | 100%    | 保守穩定，偏慢   |
| 第2階段 | 約 10-11 秒 | ~30%    | 速度提升但不穩   |
| 第3階段 | 7-9 秒   | ~70%    | 改善穩定但略不一致 |
| 第4階段 | 約 13 秒 | 100%    | 高速穩定，策略成熟 |
## 🧠 獎勵函數設計

### 設計原則：

| 評估項目           | 方法說明                                 |
|--------------------|------------------------------------------|
| 是否偏離賽道       | 若偏離賽道（is_offtrack），立即懲罰              |
| 行進方向正確性     | heading 與理想路徑夾角 > 30° 時懲罰               |
| 與理想賽道距離     | 計算實際位置至 racing line 的最短垂直距離           |
| 駕駛位置貼邊情形   | 根據彎道類型（左彎/右彎/中線）配合車輛位置給予獎勵     |
| 行駛速度           | 根據當前 waypoint 所屬區段，設定適當速度範圍與懲罰機制 |
| 是否完成賽道       | 完成賽道後依照步數給予額外獎勵                     |

---

## 🧭 賽道資料（racing_track）建構

### 目標：
建立 `[x, y, optimal_speed, expected_time]` 組成的理想路徑點序列

### 建構流程：

1. **路徑座標：** 手動或模擬導出賽道 waypoints  
2. **速度設定：**  
   - 直線：3.5 ~ 4.0 m/s  
   - 中彎：2.5 ~ 3.0 m/s  
   - 髮夾彎：1.5 ~ 2.0 m/s  

3. **時間估算：**  
   使用段距離 / 設定速度，累加預估每段耗時

### 資料格式範例：
```python
racing_track = [
    [x1, y1, 3.5, 0.14],
    [x2, y2, 2.7, 0.19],
    ...
]
```
## 🧭 建立理想路徑點序列的好處

### 精確導航參考
透過賽道上每個點的座標（x, y），車輛可以有清晰且連續的目標位置參考，避免迷失或偏離路線。

### 速度控制優化
除了位置，配合每點的最適速度（optimal_speed）可幫助車輛在不同路段（直道、彎道等）調整速度，提高安全性與效率。

### 時間管理與進度評估
利用預期時間（expected_time）能評估當前車輛速度與路徑效率，判斷是否達到理想配速，有助於設計獎勵函數以鼓勵更快完賽。

### 強化學習獎勵設計依據
提供清楚的「理想行為標準」，作為強化學習模型的獎勵基準，促進模型學習穩定且高效的駕駛策略。

### 彈性調整策略空間
可根據賽道特性或訓練需求調整速度和時間點，靈活設計不同難度及目標的訓練場景。

---

總結來說，理想路徑點序列不僅是車輛行駛的目標，也提供速度與時間維度的多重參考，提升自駕強化學習的準確性與收斂效率。

## ⚙️ 訓練參數與策略
| 參數名稱                  | 設定值    | 說明                    | 調整建議與解釋                                                                    |
| --------------------- | ------ | --------------------- | -------------------------------------------------------------------------- |
| learning\_rate        | 0.0003 | 較小學習率有助穩定訓練           | 若訓練不收斂或搖擺不定，可嘗試降低學習率（如 0.0001）；若收斂過慢，且模型表現不足，可適度提升。                        |
| batch\_size           | 64     | 單次訓練樣本量，影響收斂速度        | 較大 batch\_size 可穩定梯度估計，但會增加記憶體消耗。若訓練不穩定或結果有波動，嘗試增減 batch\_size。            |
| entropy\_target       | 0.01   | 控制探索程度，避免過度保守         | 數值越大，探索性越強。若代理過早收斂到次優策略，可增大此值促進探索；若策略變動過大導致不穩定，減小此值。                       |
| discount\_factor (γ)  | 0.99   | 強調長期獎勵                | 接近 1 時代理重視未來獎勵，適合長期任務。若想讓代理更專注於短期表現，可降低此值（例如 0.95）。                        |
| max\_episode\_steps   | 600    | 與賽道長度相符               | 需根據賽道長度及預期完賽時間調整。若代理無法完成任務，可適當延長；若訓練效率過低，嘗試縮短以加速回合結束。                      |
| clip\_range (PPO)     | 0.2    | 限制策略更新幅度以防發散          | 調整此值可控制策略更新幅度，數值過大易導致不穩定，過小可能學習過慢。一般可從 0.1 到 0.3 試驗最合適值。                   |
| reward\_scale\_factor | 1.0    | 調整 reward 在 loss 中的權重 | 透過調整該因子，可以放大或縮小 reward 對訓練損失的影響。若 reward 信號過弱，可增加此值；若 reward 過強導致訓練震盪，可降低。 |



📌 **reward function 設計思路總結**
以下為最終 reward 設計策略邏輯：

* ❗ 出界（`all_wheels_on_track = False`）立即 return `1e-3`
* ✅ 距離中心線越近 → reward 乘以 1.2~1.4
* ⚠️ 過大轉向角（`steering > 20°`）→ reward 乘以 0.6
* 🏎️ 合理速度（`1.5 ≤ speed ≤ 2.5`）→ reward 乘以 1.2
* ❌ 高速大角度轉彎 → reward 降至 0.5 以下
* 📉 速度過慢（`< 1.0`）→ reward 乘以 0.8 或直接懲罰

🏆 **最終成果**

在「正大-北科大2025春季 DeepRacer 課程計時賽」中，我們的模型 `Pacer-clone1` 取得了優異的成績：

* **當前排名：** 1 / 11
* **我的最好成績：** 00:07.863
* **出界次數：** 0
* **提交次數：** 139
* **提交時間：** 2025-05-29 13:31

這項成果證明了我們在多階段訓練中不斷優化 reward function 的有效性，成功地訓練出一個能夠在賽道上實現高速且穩定行駛的 DeepRacer 模型。

📸 **圖片參考：**
![最終成果圖表]
![alt text](image.png)