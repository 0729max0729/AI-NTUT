# 🏎️ 自動駕駛強化學習訓練報告

## 👥 組別與組員  
**組別**：第11組 
**組員名單**：  
- 楊少禎 108360224 
- 陳俊吉 111360203  
- 劉品辰 111360110 

---

## 🧠 獎勵函數設計

### 設計原則：

| 評估項目           | 方法說明                                 |
|--------------------|------------------------------------------|
| 是否偏離賽道       | 若偏離賽道（is_offtrack），立即懲罰              |
| 行進方向正確性     | heading 與理想路徑夾角 > 30° 時懲罰               |
| 與理想賽道距離     | 計算實際位置至 racing line 的最短垂直距離           |
| 駕駛位置貼邊情形   | 根據彎道類型（左彎/右彎/中線）配合車輛位置給予獎勵     |
| 行駛速度           | 根據當前 waypoint 所屬區段，設定適當速度範圍與懲罰機制 |
| 是否完成賽道       | 完成賽道後依照步數給予額外獎勵                     |

---

## 🧭 賽道資料（racing_track）建構

### 目標：
建立 `[x, y, optimal_speed, expected_time]` 組成的理想路徑點序列

### 建構流程：

1. **路徑座標：** 手動或模擬導出賽道 waypoints  
2. **速度設定：**  
   - 直線：3.5 ~ 4.0 m/s  
   - 中彎：2.5 ~ 3.0 m/s  
   - 髮夾彎：1.5 ~ 2.0 m/s  

3. **時間估算：**  
   使用段距離 / 設定速度，累加預估每段耗時

### 資料格式範例：
```python
racing_track = [
    [x1, y1, 3.5, 0.14],
    [x2, y2, 2.7, 0.19],
    ...
]
```
## 🧭 建立理想路徑點序列的好處

### 精確導航參考
透過賽道上每個點的座標（x, y），車輛可以有清晰且連續的目標位置參考，避免迷失或偏離路線。

### 速度控制優化
除了位置，配合每點的最適速度（optimal_speed）可幫助車輛在不同路段（直道、彎道等）調整速度，提高安全性與效率。

### 時間管理與進度評估
利用預期時間（expected_time）能評估當前車輛速度與路徑效率，判斷是否達到理想配速，有助於設計獎勵函數以鼓勵更快完賽。

### 強化學習獎勵設計依據
提供清楚的「理想行為標準」，作為強化學習模型的獎勵基準，促進模型學習穩定且高效的駕駛策略。

### 彈性調整策略空間
可根據賽道特性或訓練需求調整速度和時間點，靈活設計不同難度及目標的訓練場景。

---

總結來說，理想路徑點序列不僅是車輛行駛的目標，也提供速度與時間維度的多重參考，提升自駕強化學習的準確性與收斂效率。

## ⚙️ 訓練參數與策略
| 參數名稱                  | 設定值    | 說明                    | 調整建議與解釋                                                                    |
| --------------------- | ------ | --------------------- | -------------------------------------------------------------------------- |
| learning\_rate        | 0.0003 | 較小學習率有助穩定訓練           | 若訓練不收斂或搖擺不定，可嘗試降低學習率（如 0.0001）；若收斂過慢，且模型表現不足，可適度提升。                        |
| batch\_size           | 64     | 單次訓練樣本量，影響收斂速度        | 較大 batch\_size 可穩定梯度估計，但會增加記憶體消耗。若訓練不穩定或結果有波動，嘗試增減 batch\_size。            |
| entropy\_target       | 0.01   | 控制探索程度，避免過度保守         | 數值越大，探索性越強。若代理過早收斂到次優策略，可增大此值促進探索；若策略變動過大導致不穩定，減小此值。                       |
| discount\_factor (γ)  | 0.99   | 強調長期獎勵                | 接近 1 時代理重視未來獎勵，適合長期任務。若想讓代理更專注於短期表現，可降低此值（例如 0.95）。                        |
| max\_episode\_steps   | 600    | 與賽道長度相符               | 需根據賽道長度及預期完賽時間調整。若代理無法完成任務，可適當延長；若訓練效率過低，嘗試縮短以加速回合結束。                      |
| clip\_range (PPO)     | 0.2    | 限制策略更新幅度以防發散          | 調整此值可控制策略更新幅度，數值過大易導致不穩定，過小可能學習過慢。一般可從 0.1 到 0.3 試驗最合適值。                   |
| reward\_scale\_factor | 1.0    | 調整 reward 在 loss 中的權重 | 透過調整該因子，可以放大或縮小 reward 對訓練損失的影響。若 reward 信號過弱，可增加此值；若 reward 過強導致訓練震盪，可降低。 |



## 🛠️ 訓練技巧分享

### 初期訓練觀察重點：
- 確保車輛開始能貼近賽道，避免總是偏離或打轉
- 若 early reward 不穩，考慮降低 learning_rate 或放寬懲罰條件

### 獎勵函數調整流程：
- 每次只改一處條件，觀察 reward 曲線與車輛行為
- 可用視覺化工具對比不同版本表現（如貼線程度、速度分布）

### 速度調控策略：
- 搭配 waypoints 區段分類調整速度門檻
- 若 steering_angle 過大，給予速度懲罰避免過彎失控

### 完賽優化：
設計完成獎勵與步數掛鉤，例如：

```python
reward += COMPLETION_BONUS / (steps - TARGET_STEP)
