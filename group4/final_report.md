
#  第四組—AI賽車訓練報告

### 組員:陳誼庭, 陳品融, 古竹辰, 熊珮雰



## 🔧 第一階段訓練說明：獎勵函數 + 訓練策略
### 🎯 一、獎勵函數運作原理

這個獎勵函數是設計來同時強化車輛的行駛穩定性、方向正確性與速度效率：


✅ 主要獎勵邏輯：

| 評估項目          | 判斷條件                                        | 對應獎懲說明                   |
| ------------- | ------------------------------------------- | ------------------------ |
| 是否出界          | `is_offtrack` 或 `!all_wheels_on_track`      | 若出界，回傳最低分 1e-3           |
| 行駛方向是否與賽道方向一致 | `direction_diff < 8°`、`<15°`、其他             | 獎勵 1.5、1.0、0.6（方向誤差大會懲罰） |
| 是否靠近中心線       | 根據 `distance_from_center / (track_width/2)` | 小於 0.1 為最佳，愈遠懲罰愈重        |
| 速度控制          | 根據 `direction_diff` 與 `speed` 組合            | 彎道慢行給獎、彎道快懲罰，直線快給獎       |
| 轉向平穩度         | `steering_angle < 15 or >25`                | 平穩加分，劇烈轉向扣分              |
| 完成進度          | `reward += (progress / 100)`                | 推動整圈跑完，每 % 都給予獎勵         |

➡️ 核心邏輯是平衡三者：對準方向 + 靠近中心 + 速度合理 + 穩定轉向。

---

### 🧠 二、訓練設定與模型參數
| 項目              | 值                                        |
| --------------- | ---------------------------------------- |
| 環境              | re\:Invent 2018（逆時針）                     |
| 動作空間            | 連續型（速度 [0.9, 2.1] m/s、轉向角 [-25°, 25°]）      |
| 演算法             | PPO（Proximal Policy Optimization）        |
| 訓練時間            | 60 分鐘（本階段可能已執行 20 次疊代）                   |
| 框架              | Tensorflow                               |
| 重要超參數           |                                          |
| ┗ batch size    | 64                                       |
| ┗ epoch         | 4                                        |
| ┗ learning rate | 0.0003                                   |
| ┗ entropy       | 0.005（控制探索程度）                            |
| ┗ discount      | 0.95（未來獎勵折扣）                             |
| ┗ loss          | Mean squared error                       |
| ┗ 每次更新使用經驗筆數    | 18                                       |

---

### 📈 三、訓練過程觀察
根據獎勵圖：


| 指標            | 觀察結果                                 |
| ------------- | ------------------------------------ |
| **獎勵分數（綠色）**   | 呈現逐漸上升趨勢，代表模型穩定學習中。            |
| **完成百分比（紅色：評估）** | 從第 10 次之後急速上升接近 100%，顯示模型開始有效完賽。                   |
| **完成百分比（藍色：訓練）** | 持續穩定提升，未完全一致但反映整體學習趨勢。 |

![獎勵圖1](https://hackmd.io/_uploads/S14NqPeExe.png)


---

### 🔁 訓練思路建議（第一階段）
🎓 初期策略：
目標為穩定完賽，不強求最快圈速。

利用多項條件複合式獎勵（方向、位置、速度），讓車學會基本駕駛。

設定保守速度門檻（如 <1.8 m/s 在彎道較安全）避免失控。

鼓勵平穩小角度轉向，避免劇烈晃動。

🔄 接下來應對策略（第二階段預告）：
針對「速度提升」設計更積極的獎勵。

利用 track curvature 預測前方急彎，提前減速。

若模型已能穩定完賽，進入「優化圈速」階段。



## 🔧 第二階段訓練說明：獎勵函數優化 + 訓練調整
### 🎯 一、獎勵函數運作原理
相較第一階段，這次的獎勵函數更偏向「鼓勵高速直線奔馳 + 彎道安全穩定」，以下是具體說明：

✅ 獎勵邏輯一覽：
| 評估面向       | 判斷條件                                         | 獎懲策略                        |
| ---------- | -------------------------------------------- | --------------------------- |
| **出界或離線**  | `is_offtrack or not all_wheels_on_track`     | 馬上結束，回傳 `1e-3`              |
| **中心線距離**  | `distance / (track_width/2)` 分段比例            | 最中心獎勵 \*1.3，偏離最多懲罰至 \*0.5   |
| **高速直線獎勵** | `direction_diff < 8° and steering < 10` 且速度高 | 若 `speed >= 2.0`，獎勵乘以 \*2.0 |
| **彎道安全策略** | 彎道中若 `steering < 20` 且 `speed <= 1.6`        | 鼓勵安全操控，獎勵 \*1.2，否則懲罰 \*0.7  |
| **轉向平順**   | `steering < 12` or >25                       | 平穩加乘 \*1.1、劇烈懲罰 \*0.8       |
| **進度獎勵**   | `reward += (progress / 50.0)`                | 進度倍增（第一階段為 /100），鼓勵更快完賽     |

➡️ 整體特徵：偏向強化速度與進度，適合模型已具備基本完賽能力後的「性能衝刺」階段。

---

### ⚙️ 二、模型參數調整
| 參數            | 第一階段               | 第二階段            | 調整影響說明                              |
| ------------- | ------------------ | --------------- | ----------------------------------- |
| 訓練時間          | 60 分鐘              | 15 分鐘           | 減少訓練時間 → 驗證短期學習效果是否穩定               |
| 速度範圍          | \[0.9, 2.1] m/s    | \[1.3, 2.1] m/s | 提高最低速度 → 避免模型過慢、推動速度增長              |
| 轉向範圍          | \[-25, 25]°        | \[-30, 30]°     | 擴大可轉角範圍 → 更靈活轉彎，適合彎道判斷更多樣           |
| Entropy 熵值    | 0.005              | 0.0075          | 增加探索性 → 鼓勵模型在速度選擇上更大膽探索             |
| 折扣因子（γ）       | 0.95               | 0.92            | 更注重當前報酬（短期）→ 與進度倍增設計配合，加快學習完成圈速     |
| Loss Function | Mean squared error | Huber loss      | Huber 更穩健，能避免 outlier 影響，適合處理極端獎勵波動 |
| 經驗更新筆數        | 18                 | 24              | 單次更新更多樣本 → 加快收斂速度但提升不穩定性            |

---


### 📈 三、訓練成果觀察
| 數據指標     | 觀察結果說明                          |
| -------- | ------------------------------- |
| 獎勵分數（綠）  | 第 2 次運算顯著上升，代表訓練過程有效提升模型績效      |
| 訓練完成率（藍） | 小幅上升，但整體較穩定 → 暗示模型仍在探索階段        |
| 評估完成率（紅） | 起初偏高、之後略降 → 可能受訓練時間縮短或進度獎勵放大所影響 |

![獎勵圖2](https://hackmd.io/_uploads/rylljvlNxe.png)
➡️ 結論：此階段模型在短時間內有顯著進步，特別是在「平均獎勵」與「速度挑戰」方面表現提升。

---

### 📘 第二階段總結與思路

| 面向       | 說明                                |
| -------- | --------------------------------- |
| **訓練目標** | 從穩定完賽 → 邁向「更快完賽」、「更高速度」，進入性能優化階段。 |
| **策略演變** | 提高最低速度門檻、放寬轉向角度、放大進度報酬，配合更強的直線獎勵。 |
| **風險考量** | 速度快容易失控，因此增加探索熵值與轉向穩定判斷以平衡失誤風險。   |





## 🔧 第三階段訓練說明：預測彎道 + 速度極限控制

### 🎯 一、獎勵函數運作原理
本階段的獎勵函數進一步強化「預測未來彎道」與「極速行駛條件控制」，具備更高水準的駕駛智慧要求：

✅ 新增亮點邏輯：
| 評估面向           | 條件 / 判斷                          | 獎勵設計                 |
| -------------- | -------------------------------- | -------------------- |
| **未來方向預測**     | `direction_diff_ahead > 20` 且速度高 | 懲罰 \*0.7（避免高速彎道出軌）   |
| **極速獎勵**       | 當前方向差小於 10 且 `speed > 1.9`       | 給予高額乘數 \*1.8，專屬直線衝刺  |
| **直線 vs 彎道速度** | 根據 `direction_diff` 調整速度門檻       | 彎道快扣分、彎道慢鼓勵；直線快加倍    |
| **轉向平滑性**      | `steering < 15` 為平滑；`>25` 為劇烈    | 平滑加 \*1.2，劇烈懲罰 \*0.7 |
| **靠近中心線**      | 越靠近中心線獎勵越高                       | \*1.5 \~ \*0.5       |
| **進度推進**       | `reward += progress / 100.0`     | 鼓勵持續完成比賽             |

➡️ 這次最大的提升在於「前視角預測」，藉由觀察數個 waypoint 之後的角度差，預測前方是否為彎道，再主動懲罰「過快進彎」的行為，強化策略性駕駛能力。

---


### ⚙️ 二、訓練參數與模型調整
| 調整項目                       | 與前階段差異                   | 說明與意圖                |
| -------------------------- | ------------------------ | -------------------- |
| 訓練時間                       | 從 15 分鐘 ➜ 提升至 **25 分鐘**  | 延長訓練時間，有利複雜邏輯學習與收斂   |
| 熵值 Entropy                 | 從 0.0075 ➜ **提升至 0.012** | 鼓勵更多策略探索，特別是在前視彎道決策上 |
| 其他參數（如速度範圍、折扣係數、Loss、樣本數等） | 與第二階段一致                  | 表示本階段以獎勵設計為主要變因      |

---

### 📈 三、訓練成果觀察


| 指標            | 觀察結果                                 |
| ------------- | ------------------------------------ |
| **獎勵分數（綠）**   | 第 1 次疊代後大幅上升，第 3 次小幅下滑後回升            |
| **完成率（訓練，藍）** | 穩定緩步上升，表示模型行為正在穩定化                   |
| **完成率（評估，紅）** | 一開始就達高點，稍微下降後又回升至 100%，**代表策略適用性提升** |

![獎勵圖3](https://hackmd.io/_uploads/S1RqsDx4eg.png)

➡️ 模型表現顯示出：引入預測彎道與高速控制後，模型整體穩定性與完賽率皆有良好發展，顯示策略式獎勵帶來顯著提升。




### 🧠 第三階段總結與訓練思路進化

| 階段       | 重點目標           | 訓練策略演進                        |
| -------- | -------------- | ----------------------------- |
| 第一階段     | 穩定完賽           | 控制出界、對齊方向、轉向平順                |
| 第二階段     | 加速完賽           | 鼓勵高速直線行駛、放大進度獎勵、熵值稍微提升        |
| **第三階段** | **策略性駕駛與彎道預測** | 加入前視彎道偵測與預警，實作風險感知決策（像人類駕駛邏輯） |


## 🔧 第四階段訓練說明：高精度直線衝刺 + 高彎道風險控制



### 🎯 一、獎勵函數運作原理
第四階段的獎勵函數是一個 「更精緻化」的策略控制系統，特別是強化「方向對齊分級」、「速度門檻」、「彎道預判嚴懲」三個面向：

✅ 關鍵獎勵設計：
| 評估面向    | 判斷條件                                        | 獎懲策略                                         |
| ------- | ------------------------------------------- | -------------------------------------------- |
| 出界      | 出界或四輪不在線                                    | 立即結束，回傳 `1e-3`                               |
| 方向對齊強化  | `direction_diff` 分級：<5°, <10°, <15°, 其他     | 強化區隔：\*1.8、\*1.4、\*1.0、\*0.5                 |
| 中心線靠近強化 | `center_ratio` 分級：≤5%, ≤10%, ≤25%, ≤40%, 其他 | 更細緻懲獎：\*1.6 \~ \*0.4，推進車輛貼近理想線               |
| 高速直線鼓勵  | 若方向 + 未來方向皆穩，且速度 > 2.0 m/s                  | 極高獎勵 \*2.0，鼓勵在直線段大膽衝刺                        |
| 彎道過快懲罰  | 若目前或未來方向偏差大，且速度 > 1.7 m/s                   | 大幅懲罰 \*0.6；未來方向 >25° 時且速度 >1.5 m/s 再懲罰 \*0.5 |
| 彎道穩定鼓勵  | 若偏差大但速度低                                    | 鼓勵穩定駕駛 \*1.2                                 |
| 平滑轉向控制  | 轉向角 <10, <20, 否則                            | \*1.3、\*1.0、\*0.6，提升方向穩定性                    |
| 完賽進度    | `progress / 100`                            | 鼓勵持續推進比賽                                     |
➡️ 此設計等同於**「專業駕駛員」邏輯判斷**，對直線與彎道有明確策略區隔，且風險預判能力大幅提升。



---


### ⚙️ 二、訓練參數與模型演進
| 參數名稱    | 調整值              | 相較前三階段變化與意圖                    |
| ------- | ---------------- | ------------------------------ |
| 訓練時間    | **60 分鐘**        | 拉長訓練時間，適應更複雜的獎勵函數收斂需求          |
| 速度範圍    | `[1.3, 2.4] m/s` | 上限提高至 2.4 m/s，提升極速挑戰潛力         |
| 轉向角度    | `[-30°, 30°]`    | 維持靈活的彎道應對能力                    |
| 熵值（探索度） | **0.02**         | 大幅提升探索 → 模型更積極嘗試新策略以應對複雜情境     |
| Epochs  | **6** 次          | 從 4 → 6，允許每批資料訓練更充分，提升穩定性與泛化能力 |
| 折扣係數 γ  | 0.95             | 保持穩定，維持長期報酬導向                  |
| 損失函數    | Huber Loss       | 抗極端值影響，保持穩健更新                  |


---

### 📈 三、訓練結果分析
| 項目           | 觀察結果說明                               |
| ------------ | ------------------------------------ |
| **獎勵分數（綠）**  | 前期浮動，第 5\~6 次明顯提升，後期再度穩定上升 → 表現優化期出現 |
| **訓練完成率（藍）** | 整體穩定成長，代表模型正在逐步學會「策略性完成比賽」           |
| **評估完成率（紅）** | 第 2 次疊代即大幅達標，後期穩定維持接近 100% → 模型泛化表現佳 |

![獎勵圖4](https://hackmd.io/_uploads/SkXWRwlNlg.png)


➡️ 可見這個訓練階段成功強化了模型「穩定完賽 + 加速衝刺」的能力，且泛化能力已成熟。

---


### 🧠 第四階段訓練重點與優勢

| 面向         | 說明                                |
| ---------- | --------------------------------- |
| **獎勵函數特色** | 引入更多「風險控制」與「預判策略」，並加強區間細化獎懲邏輯     |
| **訓練目標轉變** | 從純完賽與速度提升 → 進化為「風險/獎勵平衡」與「策略切換能力」 |
| **適用情境**   | 可因應多彎賽道、極限速度場景、甚至變化型賽道            |

## 🔧 第五階段訓練說明：區段策略行駛 + 高穩定完成率優化
### 🎯 一、獎勵函數進化
第五階段除了保留前面幾階段的關鍵策略，還導入了「特定彎道段落之行駛側判斷」，這是一種 區段式策略強化學習 概念的實踐。

✅ 新增/強化邏輯彙整：
| 面向                 | 判斷條件                                          | 獎懲設計說明                             |
| ------------------ | --------------------------------------------- | ---------------------------------- |
| **出界/車輪不在線**       | `is_offtrack or !all_wheels_on_track`         | 馬上結束，回傳 1e-3                       |
| **中心線距離獎勵**        | `center_ratio ≤ 0.05 → *1.6`，>0.4 → \*0.4     | 與前階段一致，保持穩定中線駕駛                    |
| **方向對齊強化**         | `<5° → *1.6，<10° → *1.3，其他懲罰`                 | 更精細門檻、鼓勵更準確方向控制                    |
| **lookahead 彎道預判** | `direction_diff_ahead > 25 且速度高` → 懲罰 \*0.6   | 彎道預測搭配速度判斷 → 預防失控                  |
| **直線高速鼓勵**         | `direction_diff_ahead < 10 且速度 > 2.0` → \*1.8 | 更積極衝刺獎勵，專屬直線區域                     |
| **轉向平穩度獎勵**        | `steering <10 → *1.3，>20 → *0.7`              | 持續懲罰劇烈轉向行為                         |
| **進階新增：區段策略**      | 針對部分 waypoint（如 22~~39、76~~91、100\~111）       | 若在這些區段時靠正確側行駛（左彎靠左 / 右彎靠右） → \*1.2 |
| **進度推進獎勵**         | `reward += progress / 100`                    | 保持循環結束推進誘因                         |

➡️ 總結亮點：這階段導入「分段區域路線選擇」，等同人類賽車手在進彎時提前選好彎心位置，屬於高階駕駛邏輯。


---


### ⚙️ 二、訓練參數總覽
| 設定項目                     | 調整內容             | 與前階段比較與意圖             |
| ------------------------ | ---------------- | --------------------- |
| 訓練時間                     | **70 分鐘**        | 最長訓練，為了適應更多樣路況與行為策略收斂 |
| 速度範圍                     | `[1.2, 2.4] m/s` | 提高最低速度限制，鼓勵更快基本速度     |
| 熵值（探索性）                  | 0.02             | 維持較高探索性，利於嘗試新區段路線策略   |
| 更新經驗筆數                   | **32（提升）**       | 增加策略樣本數，有助於高泛化性能      |
| 其他參數（learning rate、loss） | 與第四階段一致          | 保持穩定收斂效果              |


---


### 📈 三、訓練成效觀察

| 指標            | 分析結果                                       |
| ------------- | ------------------------------------------ |
| **完成率（評估紅色）** | **連續保持 100%**，代表模型泛化成功，能穩定完賽               |
| **訓練完成率（藍色）** | 高於 75%，整體波動小 → 學習穩定，不容易 overfit            |
| **獎勵分數（綠色）**  | 穩定高分（平均約 220\~250），部分疊代略低，反映在訓練策略中的探索與調整過程 |

![獎勵圖5](https://hackmd.io/_uploads/BJ23Cwl4gl.png)


---


### 🧠 第五階段重點結論

| 面向       | 內容說明                             |
| -------- | -------------------------------- |
| **學習重點** | 導入「特定區段正確側行駛」策略，逼近真實駕駛員的路線選擇決策邏輯 |
| **訓練目標** | 穩定完賽 + 速度極大化 + 彎道預測決策 + 路線選邊最佳化  |
| **實作價值** | 模型具備應對複雜多彎道的能力，具備實用部署潛力          |

## 🔧 第六階段訓練重點與優勢

### 🧠 一、獎勵函數邏輯分析
這一版獎勵函數幾乎是承襲第五階段設計，主軸依然是「方向對齊 + 彎道預測 + 中心控制 + 特定區段策略」。具體結構如下：

✅ 功能對照表：
| 模組名稱               | 說明                                                    |
| ------------------ | ----------------------------------------------------- |
| **出界處理**           | `if is_offtrack or !all_wheels_on_track: return 1e-3` |
| **方向對齊獎勵**         | 小於 5° 獎勵 \*1.6，>15° 懲罰 \*0.6                          |
| **中心線獎勵**          | ≤5%：\*1.6、≤40%：\*0.7、>40%：\*0.4                       |
| **lookahead 彎道預測** | `direction_diff_ahead` 若 >25° 且速度高 → 懲罰 \*0.6         |
| **直線高速鼓勵**         | `diff_ahead < 10 且 speed > 2.0` → 獎勵 \*1.8            |
| **平滑轉向獎勵**         | `steering < 10°` → \*1.3；>20° → \*0.7                 |
| **區段策略靠邊**         | waypoint 落在指定區間時靠內側行駛 → \*1.2                         |
| **進度推進獎勵**         | `reward += progress / 100.0`                          |

🎯 關鍵點：本函數保持第五階段完整邏輯，但模型結果顯示出「表現下滑」，問題可能不在函數本身，而是訓練參數與環境設定的改動（見下方分析）。

---


### ⚙️ 二、訓練參數設定
| 訓練參數            | 值                | 與第五階段比較            |
| --------------- | ---------------- | ------------------ |
| **訓練時間**        | **20 分鐘**        | ↓ 明顯減少（第5階段為 70 分） |
| **速度範圍**        | `[1.2, 2.8] m/s` | ↑ 最大速度提高（風險增加）     |
| **轉向角**         | `[-30°, 30°]`    | 保持                 |
| **探索熵 Entropy** | `0.02`           | 保持高探索              |
| **折扣係數 γ**      | `0.95`           | 保持                 |
| **更新經驗筆數**      | `32`             | 保持                 |

📌 潛在問題：

訓練時間明顯不足，難以收斂。

最大速度上限提升至 2.8 m/s，但獎勵函數仍用 2.0 m/s 為門檻，導致高速無明確激勵。

---


### 📈 三、訓練成果觀察

| 指標             | 結果說明                    |
| -------------- | ----------------------- |
| **完成率（紅色：評估）** | 從 100% ➜ 90%，雖仍高但已有下降   |
| **訓練完成率（藍色）**  | 小幅下降，顯示模型有失誤情況          |
| **平均獎勵（綠色）**   | 明顯下降（從 220 ➜ 180），回報率變差 |

![獎勵圖6](https://hackmd.io/_uploads/Sy0C-Oe4ge.png)



➡️ 總結現象：雖使用相同獎勵策略，但在短時間與更高速度的壓力下，模型尚未學會有效應對新速度邏輯，導致表現下滑。

---


### 🧾 四、第六階段小結與調整建議
| 類別      | 建議方向                                      |
| ------- | ----------------------------------------- |
| ✅ 獎勵邏輯  | 已完善，不需大幅更動                                |
| ⏳ 訓練時間  | 應延長至至少 40\~60 分鐘以對應高速度探索難度                |
| 🏁 獎勵門檻 | 建議將高速獎勵門檻從 `speed > 2.0` 調高至 `>2.4` 或分級給獎 |
| 🧠 模型特化 | 若主打「極速模式」，應設計新版本獎勵邏輯專攻高速轉向策略              |


## 🏁 AI 賽車訓練六階段總結報告
本報告整合六個訓練階段的獎勵函數設計邏輯、訓練參數、訓練成果與策略進化，方便回顧訓練歷程與未來調整方向。

### 🚦 第一階段：穩定完賽訓練

---

* 目標：學習基本方向控制、穩定駕駛、避免出界
* 獎勵重點：方向準確、小轉向、靠近中心、出界懲罰
* 訓練時間：60 分鐘，速度 0.9~2.1 m/s，轉向 ±25°
* 成果：模型學會穩定完賽，完成率達 100%，獎勵穩定上升

### 🛣 第二階段：直線加速優化

---

* 目標：鼓勵模型在方向準確時提升車速
* 新增邏輯：直線加速獎勵、彎道過快懲罰、轉向平穩控制
* 訓練時間縮短至 15 分鐘，速度下限提高至 1.3 m/s，熵值微升
* 成果：完成率接近 95%，平均獎勵明顯上升，訓練更具挑戰

### 🧭 第三階段：彎道預判策略

---

* 目標：加入對「未來方向」的預判，控制彎道進彎速度
* 新增邏輯：direction_diff_ahead，強化彎道風險感知
* 訓練時間：25 分鐘，保持速度與轉向，熵值再次提升至 0.012
* 成果：完成率高達 100%，平均獎勵提升至 140，模型策略性提升

### 🎯 第四階段：高精準駕駛控制

---

* 目標：強化多段式方向與中心判斷邏輯，提高駕駛精準度
* 新增邏輯：方向/中心/轉向分級獎懲，速度搭配 lookahead 判斷
* 訓練時間提升至 60 分鐘，最大速度升至 2.4 m/s，熵值 0.02
* 成果：完成率穩定 100%，平均獎勵提升至 160，行為穩定成熟

### 🧠 第五階段：彎段策略靠邊行駛

---

* 目標：引入特定 waypoint 區段的「靠內彎」策略
* 新增邏輯：判斷彎道區段 + 彎向靠邊判斷（內彎靠內）
* 訓練時間拉長至 70 分鐘，經驗筆數擴增為 32，速度上限持平
* 成果：完成率 100%、平均獎勵達 230，模型駕駛邏輯最成熟

### 🚀 第六階段：極速挑戰與不足收斂

---

* 目標：挑戰更高速度（2.8 m/s）下的穩定控制
* 沿用第五階段獎勵邏輯，惟訓練時間縮短、速度上限調高
* 訓練時間僅 20 分鐘，熵值維持 0.02，未同步調整高速獎勵條件
* 成果：完成率下降至 90%，平均獎勵下降至 180，模型未完全適應新速度

## 📊 總體觀察建議
訓練時間與模型表現正相關，請避免過度壓縮時間

高速挑戰時需同步調整獎勵門檻與風險判斷

「靠邊行駛」、「彎道預測」為提升策略性關鍵獎勵模組

可針對彎道特性設計更多地圖依賴性 reward function
